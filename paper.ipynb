{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RlNfQLi3H02"
      },
      "source": [
        "Given $q \\in \\mathbb{R}^d, K \\in \\mathbb{R}^{s \\times d}, V \\in \\mathbb{R}^{s \\times d}$, we define\n",
        "$$\\alpha_i = q^TK_i$$\n",
        "$$\\text{Attn}(q, K, V) = \\frac{e^{\\alpha_1} V_1 + \\cdots + e^{\\alpha_n} V_n}{e^{\\alpha_1} + \\cdots + e^{\\alpha_n}}$$\n",
        "We can compute $\\text{Attn}(q, K, V)$ iteratively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "s9riHhPN3JSf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "d = 5\n",
        "s = 7\n",
        "\n",
        "q = np.random.random(d)\n",
        "K = np.random.random((s, d))\n",
        "V = np.random.random((s, d))\n",
        "\n",
        "num = 0\n",
        "den = 0\n",
        "\n",
        "for k, v in zip(K, V):\n",
        "  alpha = (q * k).sum()\n",
        "  num += np.exp(alpha) * v  # e^{alpha_i}V_i\n",
        "  den += np.exp(alpha)  # e^{alpha_i}\n",
        "\n",
        "attn_output = num / den\n",
        "# e^{alpha_1}V_1 + ... + e^{alpha_n}V_n\n",
        "# -------------------------------------\n",
        "#    e^{alpha_1} + ... + e^{alpha_n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51s8adIA6mvj"
      },
      "source": [
        "This is equivalent to Blockwise Parallel, applied to a single query vector, with a chunk-size of 1 and without the $\\max_i$ term. We can check that `attn_output` is equal to traditional Attn(Q,K,V) computation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVgWzioc4cVP",
        "outputId": "6efcdb69-266e-4293-ac53-53167382255e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "attn_weights = softmax(np.einsum('d,sd -> s', q, K), -1)  # q^T K\n",
        "attn_output2 = np.einsum('s,sd -> d', attn_weights, V) # q^T K\n",
        "np.allclose(attn_output, attn_output2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQeV56VC61Rc"
      },
      "source": [
        "Next we introduce $\\max_i$ to improve floating point stability. In order to show the math more clearly, we will walk through the first two steps of the iteration, using numeric suffixes to prevent variables from shadowing each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "pJX6NlFz5R45"
      },
      "outputs": [],
      "source": [
        "num0 = 0\n",
        "den0 = 0\n",
        "max0_i = -np.inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the logic that we perform in the first iteration of the loop, comparing the query vector with the first (size-1) chunk of the key matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHWIJmjP6-uk",
        "outputId": "43213b03-38a2-4861-98ed-a3cf70145663"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True])"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v0 = V[0]\n",
        "k0 = K[0]\n",
        "alpha0 = (q * k0).sum()\n",
        "max1_i = max(alpha0, max0_i)\n",
        "num1 = num0 * np.exp(max0_i - max1_i) + np.exp(alpha0 - max1_i) * v0\n",
        "num1 == np.exp(alpha0 - max1_i) * v0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2i7cbYJ8xoi"
      },
      "source": [
        "Since $\\texttt{num0} = 0$,\n",
        "$\\texttt{num1} = e^{\\alpha_0 - \\max0_i}v_0$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGczEdA47OVz",
        "outputId": "d54fb96a-9014-45db-c75f-6b785b8819cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "den1 = den0 * np.exp(max0_i - max1_i) + np.exp(alpha0 - max1_i)\n",
        "den1 == np.exp(alpha0 - max1_i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E7Z4YeQ8Tri"
      },
      "source": [
        "Similarly, since $\\texttt{den0} = 0$, $\\texttt{den1} = e^{\\alpha_0 - \\max1_i}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSU3Wlic77TJ",
        "outputId": "c0bfbc0b-e760-45d4-ba24-e990afa88563"
      },
      "outputs": [],
      "source": [
        "v1 = V[1]\n",
        "k1 = K[1]\n",
        "alpha1 = (q * k1).sum()\n",
        "max_i2 = max(alpha1, max1_i)\n",
        "num2 = num1 * np.exp(max1_i - max_i2) + np.exp(alpha1 - max_i2) * v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Substituting in the value of $\\texttt{num1}$, we get:\n",
        "$$\\texttt{num2} = e^{\\alpha_0 - \\max0_i}v_0 \\times e^{\\max0_i - \\max1_i} + e^{\\alpha_1 - \\max1_i}v_1$$\n",
        "And simplifying the exponents:\n",
        "$$ = e^{\\alpha_0 - \\max1_i}v_0 + e^{\\alpha_1 - \\max1_i}v_1$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.all(\n",
        "    num2 == np.exp(alpha0 - max_i2) * v0 + np.exp(alpha1 - max_i2) * v1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvWl8qvR8DOo",
        "outputId": "d10bff61-96f4-44db-b982-ff1e01dbc58b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "den2 = den1 * np.exp(max1_i - max_i2) + np.exp(alpha1 - max_i2)\n",
        "den2 == np.exp(alpha0 - max_i2) + np.exp(alpha1 - max_i2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This second equality comes from the fact that when we substite in the value of $\\texttt{den1}$, we get:\n",
        "$$\\texttt{den2} = e^{\\alpha_0 - \\max0_i} \\times e^{\\max0_i - \\max1_i} + e^{\\alpha_1 - \\max1_i}$$\n",
        "And again simplifying the exponents:\n",
        "$$ = e^{\\alpha_0 - \\max1_i} + e^{\\alpha_1 - \\max1_i}$$\n",
        "\n",
        "When we take tha fraction of $\\texttt{num2}$ and $\\texttt{den2}$, the $\\max1_i$ terms cancel out:\n",
        " $$\\frac{\\text{num2}}{\\text{den2}} \n",
        " = \\frac{e^{\\alpha_0 - \\max_i}v_0 + e^{\\alpha_1 - \\max_i}v_1}{e^{\\alpha_0 - \\max_i} + e^{\\alpha_1 - \\max_i}}\n",
        " = \\frac{e^{\\alpha_0} v_0 + e^{\\alpha_1} v_1}{e^{\\alpha_0} + e^{\\alpha_1}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIlrfDq9DYV",
        "outputId": "3b1358cd-5d2b-420d-d28c-a7f9005cbbbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.allclose(\n",
        "    num2 / den2,\n",
        "    (np.exp(alpha0) * v0 + np.exp(alpha1) * v1) / (np.exp(alpha0) + np.exp(alpha1))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that this previous expression looks like traditional Attn(Q,K,V). We can confirm the equivalence as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgSFJ05QRV8Z",
        "outputId": "2160e773-3a51-4b70-d48d-f4422adf62ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alpha = np.array([alpha0, alpha1])\n",
        "v = np.array([v0, v1])\n",
        "np.allclose(\n",
        "    num2 / den2,\n",
        "    (softmax(alpha)[..., None] * v).sum(0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " $$\\frac{\\text{num2}}{\\text{den2}} = \\text{softmax}(\\alpha_0, \\alpha_1)^T[ v_0, v_1 ]$$\n",
        "\n",
        " Hopefully this suffices to show the equivalence between Blockwise Parallel and traditional Attn(Q,K,V) computation. We now add the following logic:\n",
        " - iteration over chunks of the query vector\n",
        " - chunk-size > 1\n",
        " - a batch dimension which might also include the head dimension\n",
        "\n",
        "We also put this logic into a loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "lgYpN68O2Jvu"
      },
      "outputs": [],
      "source": [
        "num0 = 0\n",
        "den0 = 0\n",
        "max_i0 = -np.inf\n",
        "n = 3  # number of chunks\n",
        "b = 2  # batch dimension (could also include head dimension, since heads are parallel for self-attention)\n",
        "s = 7\n",
        "d = 5\n",
        "Q = np.random.random((n, b, s, d))\n",
        "K = np.random.random((n, b, s, d))\n",
        "V = np.random.random((n, b, s, d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "xZz1vfDM-fdG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2, 7, 5)"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn_outputs = []\n",
        "\n",
        "q: np.ndarray\n",
        "for i, q in enumerate(Q):\n",
        "  assert list(q.shape) == [b, s, d]\n",
        "  num = np.zeros((b,s,d))  # initialize numerator\n",
        "  den = np.zeros((b,s))  # initialize denominator\n",
        "  max_i = -np.inf * np.ones((b, s))  # initialize max_i\n",
        "\n",
        "  k: np.ndarray\n",
        "  v: np.ndarray\n",
        "  for j, (k, v) in enumerate(zip(K, V)):\n",
        "    assert list(k.shape) == [b, s, d]\n",
        "    assert list(v.shape) == [b, s, d]\n",
        "    alpha: np.ndarray = np.einsum('bqd,bkd -> bqk', q, k)  # q^T K\n",
        "    prev = max_i\n",
        "    max_i = np.maximum(alpha.max(-1), max_i)  # update max_i\n",
        "    exp_values = np.einsum('bqk,bkd -> bqd', np.exp(alpha - max_i[..., None]), v)  # e^{alpha - max_i}^T v\n",
        "\n",
        "    # update numerator and denominator\n",
        "    num = num * np.exp(prev - max_i)[..., None] + exp_values  \n",
        "    den = den * np.exp(prev - max_i) + np.exp(alpha - max_i[..., None]).sum(-1)\n",
        "\n",
        "  attn_outputs.append(num / den[..., None])\n",
        "\n",
        "attn_outputs = np.stack(attn_outputs)\n",
        "attn_outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compare this to a traditional Attn(Q,K,V) computation to verify that the two are equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiaKEIkR-y_6",
        "outputId": "14d9c694-1455-41f6-ff63-dd97b49f8f01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q1 = Q.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "K1 = K.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "V1 = V.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "attn_weights: np.ndarray = softmax(np.einsum('bqd,bkd -> bqk', Q1, K1), -1)  # Q^T K\n",
        "assert list(attn_weights.shape) == [b, s * n, s * n]\n",
        "attn_outputs2 = np.einsum('bqk,bkd -> bqd', attn_weights, V1)  # q^T K V\n",
        "attn_outputs = attn_outputs.transpose(1, 0, 2, 3).reshape(b, n*s, d)  # merge blocks for comparison\n",
        "np.allclose(attn_outputs, attn_outputs2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "w1 = np.random.random((d, d))\n",
        "w2 = np.random.random((d, d))\n",
        "relu = lambda x: np.maximum(0, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2, 7, 5)"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn_outputs = []\n",
        "\n",
        "q: np.ndarray\n",
        "for i, q in enumerate(Q):\n",
        "    assert list(q.shape) == [b, s, d]\n",
        "    num = np.zeros((b,s,d))  # initialize numerator\n",
        "    den = np.zeros((b,s))  # initialize denominator\n",
        "    max_i = -np.inf * np.ones((b, s))  # initialize max_i\n",
        "\n",
        "    k: np.ndarray\n",
        "    v: np.ndarray\n",
        "    for j, (k, v) in enumerate(zip(K, V)):\n",
        "        assert list(k.shape) == [b, s, d]\n",
        "        assert list(v.shape) == [b, s, d]\n",
        "        alpha: np.ndarray = np.einsum('bqd,bkd -> bqk', q, k)  # q^T K\n",
        "        prev = max_i\n",
        "        max_i = np.maximum(alpha.max(-1), max_i)  # update max_i\n",
        "        exp_values = np.einsum('bqk,bkd -> bqd', np.exp(alpha - max_i[..., None]), v)  # e^{alpha - max_i}^T v\n",
        "\n",
        "        # update numerator and denominator\n",
        "        num = num * np.exp(prev - max_i)[..., None] + exp_values  \n",
        "        den = den * np.exp(prev - max_i) + np.exp(alpha - max_i[..., None]).sum(-1)\n",
        "\n",
        "    chunk_attn_output = num / den[..., None]\n",
        "\n",
        "    ################## NEW CODE ##################\n",
        "\n",
        "    # 2-layer feedforward network\n",
        "    resid_attn = np.einsum('bqd,dw -> bqw', chunk_attn_output, w1)\n",
        "    resid_attn = relu(resid_attn)\n",
        "    resid_attn = np.einsum('bqd,dw -> bqw', resid_attn, w2)\n",
        "\n",
        "    # residual connection\n",
        "    chunk = chunk_attn_output + resid_attn + q\n",
        "    \n",
        "    ################ END NEW CODE ################\n",
        "\n",
        "    attn_outputs.append(chunk_attn_output)\n",
        "\n",
        "attn_outputs = np.stack(attn_outputs)\n",
        "attn_outputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn_weights: np.ndarray = softmax(np.einsum('bqd,bkd -> bqk', Q1, K1), -1)  # Q^T K\n",
        "assert list(attn_weights.shape) == [b, s * n, s * n]\n",
        "attn_outputs2 = np.einsum('bqk,bkd -> bqd', attn_weights, V1)  # q^T K V\n",
        "\n",
        "# 2-layer feedforward network\n",
        "resid_attn = np.einsum('bqd,dw -> bqw', attn_outputs2, w1)\n",
        "resid_attn = relu(resid_attn)\n",
        "resid_attn = np.einsum('bqd,dw -> bqw', resid_attn, w2)\n",
        "\n",
        "# residual connection\n",
        "chunk = attn_outputs2 + resid_attn + Q1\n",
        "\n",
        "attn_outputs = attn_outputs.transpose(1, 0, 2, 3).reshape(b, n*s, d)  # merge blocks for comparison\n",
        "np.allclose(attn_outputs, attn_outputs2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
