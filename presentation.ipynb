{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RlNfQLi3H02"
      },
      "source": [
        "Given $q \\in \\mathbb{R}^d, K \\in \\mathbb{R}^{s \\times d}, V \\in \\mathbb{R}^{s \\times d}$, we define\n",
        "$$\\alpha_i = q^TK_i$$\n",
        "$$\\text{Attn}(q, K, V) = \\frac{e^{\\alpha_1} V_1 + \\cdots + e^{\\alpha_n} V_n}{e^{\\alpha_1} + \\cdots + e^{\\alpha_n}}$$\n",
        "We can compute $\\text{Attn}(q, K, V)$ iteratively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s9riHhPN3JSf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "d = 5\n",
        "s = 7\n",
        "\n",
        "q = np.random.random(d)\n",
        "K = np.random.random((s, d))\n",
        "V = np.random.random((s, d))\n",
        "\n",
        "num = 0\n",
        "den = 0\n",
        "\n",
        "for k, v in zip(K, V):\n",
        "  alpha = (q * k).sum()\n",
        "  num += np.exp(alpha) * v  # e^{alpha_i}V_i\n",
        "  den += np.exp(alpha)  # e^{alpha_i}\n",
        "\n",
        "attn_output = num / den\n",
        "# e^{alpha_1}V_1 + ... + e^{alpha_n}V_n\n",
        "# -------------------------------------\n",
        "#    e^{alpha_1} + ... + e^{alpha_n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51s8adIA6mvj"
      },
      "source": [
        "This is equivalent to Blockwise Parallel, applied to a single query vector, with a chunk-size of 1 and without the $\\max_i$ term. We can check that `attn_output` is equal to traditional Attn(Q,K,V) computation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVgWzioc4cVP",
        "outputId": "6efcdb69-266e-4293-ac53-53167382255e"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "attn_weights = softmax(np.einsum('d,sd -> s', q, K), -1)  # q^T K\n",
        "attn_output2 = np.einsum('s,sd -> d', attn_weights, V) # q^T K\n",
        "assert np.allclose(attn_output, attn_output2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQeV56VC61Rc"
      },
      "source": [
        "Next we introduce $\\max_i$ to improve floating point stability. In order to show the math more clearly, we will walk through the first two steps of the iteration, using numeric suffixes to prevent variables from shadowing each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pJX6NlFz5R45"
      },
      "outputs": [],
      "source": [
        "num0 = 0\n",
        "den0 = 0\n",
        "max0_i = -np.inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the logic that we perform in the first iteration of the loop, comparing the query vector with the first (size-1) chunk of the key matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHWIJmjP6-uk",
        "outputId": "43213b03-38a2-4861-98ed-a3cf70145663"
      },
      "outputs": [],
      "source": [
        "v0 = V[0]\n",
        "k0 = K[0]\n",
        "alpha0 = (q * k0).sum()\n",
        "max1_i = max(alpha0, max0_i)\n",
        "num1 = num0 * np.exp(max0_i - max1_i) + np.exp(alpha0 - max1_i) * v0\n",
        "assert np.all(num1 == np.exp(alpha0 - max1_i) * v0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2i7cbYJ8xoi"
      },
      "source": [
        "Since $\\texttt{num0} = 0$,\n",
        "$\\texttt{num1} = e^{\\alpha_0 - \\max0_i}v_0$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGczEdA47OVz",
        "outputId": "d54fb96a-9014-45db-c75f-6b785b8819cc"
      },
      "outputs": [],
      "source": [
        "den1 = den0 * np.exp(max0_i - max1_i) + np.exp(alpha0 - max1_i)\n",
        "assert den1 == np.exp(alpha0 - max1_i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E7Z4YeQ8Tri"
      },
      "source": [
        "Similarly, since $\\texttt{den0} = 0$, $\\texttt{den1} = e^{\\alpha_0 - \\max1_i}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSU3Wlic77TJ",
        "outputId": "c0bfbc0b-e760-45d4-ba24-e990afa88563"
      },
      "outputs": [],
      "source": [
        "v1 = V[1]\n",
        "k1 = K[1]\n",
        "alpha1 = (q * k1).sum()\n",
        "max_i2 = max(alpha1, max1_i)\n",
        "num2 = num1 * np.exp(max1_i - max_i2) + np.exp(alpha1 - max_i2) * v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Substituting in the value of $\\texttt{num1}$, we get:\n",
        "$$\\texttt{num2} = e^{\\alpha_0 - \\max0_i}v_0 \\times e^{\\max0_i - \\max1_i} + e^{\\alpha_1 - \\max1_i}v_1$$\n",
        "And simplifying the exponents:\n",
        "$$ = e^{\\alpha_0 - \\max1_i}v_0 + e^{\\alpha_1 - \\max1_i}v_1$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert np.all(\n",
        "    num2 == np.exp(alpha0 - max_i2) * v0 + np.exp(alpha1 - max_i2) * v1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvWl8qvR8DOo",
        "outputId": "d10bff61-96f4-44db-b982-ff1e01dbc58b"
      },
      "outputs": [],
      "source": [
        "den2 = den1 * np.exp(max1_i - max_i2) + np.exp(alpha1 - max_i2)\n",
        "assert den2 == np.exp(alpha0 - max_i2) + np.exp(alpha1 - max_i2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This second equality comes from the fact that when we substite in the value of $\\texttt{den1}$, we get:\n",
        "$$\\texttt{den2} = e^{\\alpha_0 - \\max0_i} \\times e^{\\max0_i - \\max1_i} + e^{\\alpha_1 - \\max1_i}$$\n",
        "And again simplifying the exponents:\n",
        "$$ = e^{\\alpha_0 - \\max1_i} + e^{\\alpha_1 - \\max1_i}$$\n",
        "\n",
        "When we take tha fraction of $\\texttt{num2}$ and $\\texttt{den2}$, the $\\max1_i$ terms cancel out:\n",
        " $$\\frac{\\text{num2}}{\\text{den2}} \n",
        " = \\frac{e^{\\alpha_0 - \\max_i}v_0 + e^{\\alpha_1 - \\max_i}v_1}{e^{\\alpha_0 - \\max_i} + e^{\\alpha_1 - \\max_i}}\n",
        " = \\frac{e^{\\alpha_0} v_0 + e^{\\alpha_1} v_1}{e^{\\alpha_0} + e^{\\alpha_1}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIlrfDq9DYV",
        "outputId": "3b1358cd-5d2b-420d-d28c-a7f9005cbbbc"
      },
      "outputs": [],
      "source": [
        "assert np.allclose(\n",
        "    num2 / den2,\n",
        "    (np.exp(alpha0) * v0 + np.exp(alpha1) * v1) / (np.exp(alpha0) + np.exp(alpha1))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that this previous expression looks like traditional Attn(Q,K,V). We can confirm the equivalence as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgSFJ05QRV8Z",
        "outputId": "2160e773-3a51-4b70-d48d-f4422adf62ec"
      },
      "outputs": [],
      "source": [
        "alpha = np.array([alpha0, alpha1])\n",
        "v = np.array([v0, v1])\n",
        "assert np.allclose(\n",
        "    num2 / den2,\n",
        "    (softmax(alpha)[..., None] * v).sum(0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " $$\\frac{\\text{num2}}{\\text{den2}} = \\text{softmax}(\\alpha_0, \\alpha_1)^T[ v_0, v_1 ]$$\n",
        "\n",
        " Hopefully this suffices to show the equivalence between Blockwise Parallel and traditional Attn(Q,K,V) computation. We now add the following logic:\n",
        " - iteration over chunks of the query vector\n",
        " - chunk-size > 1\n",
        " - a batch dimension which might also include the head dimension\n",
        "\n",
        "We also put this logic into a loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lgYpN68O2Jvu"
      },
      "outputs": [],
      "source": [
        "num0 = 0\n",
        "den0 = 0\n",
        "max_i0 = -np.inf\n",
        "n = 3  # number of chunks\n",
        "b = 2  # batch dimension (could also include head dimension, since heads are parallel for self-attention)\n",
        "s = 7\n",
        "d = 5\n",
        "Q = np.random.random((n, b, s, d))\n",
        "K = np.random.random((n, b, s, d))\n",
        "V = np.random.random((n, b, s, d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xZz1vfDM-fdG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2, 7, 5)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn_outputs = []\n",
        "\n",
        "q: np.ndarray\n",
        "for i, q in enumerate(Q):\n",
        "  assert list(q.shape) == [b, s, d]\n",
        "  num = np.zeros((b,s,d))  # initialize numerator\n",
        "  den = np.zeros((b,s))  # initialize denominator\n",
        "  max_i = -np.inf * np.ones((b, s))  # initialize max_i\n",
        "\n",
        "  k: np.ndarray\n",
        "  v: np.ndarray\n",
        "  for j, (k, v) in enumerate(zip(K, V)):\n",
        "    assert list(k.shape) == [b, s, d]\n",
        "    assert list(v.shape) == [b, s, d]\n",
        "    alpha: np.ndarray = np.einsum('bqd,bkd -> bqk', q, k)  # q^T K\n",
        "    prev = max_i\n",
        "    max_i = np.maximum(alpha.max(-1), max_i)  # update max_i\n",
        "    exp_values = np.einsum('bqk,bkd -> bqd', np.exp(alpha - max_i[..., None]), v)  # e^{alpha - max_i}^T v\n",
        "\n",
        "    # update numerator and denominator\n",
        "    num = num * np.exp(prev - max_i)[..., None] + exp_values  \n",
        "    den = den * np.exp(prev - max_i) + np.exp(alpha - max_i[..., None]).sum(-1)\n",
        "\n",
        "  attn_outputs.append(num / den[..., None])\n",
        "\n",
        "attn_outputs = np.stack(attn_outputs)\n",
        "attn_outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compare this to a traditional Attn(Q,K,V) computation to verify that the two are equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiaKEIkR-y_6",
        "outputId": "14d9c694-1455-41f6-ff63-dd97b49f8f01"
      },
      "outputs": [],
      "source": [
        "Q1 = Q.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "K1 = K.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "V1 = V.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "attn_weights: np.ndarray = softmax(np.einsum('bqd,bkd -> bqk', Q1, K1), -1)  # Q^T K\n",
        "assert list(attn_weights.shape) == [b, s * n, s * n]\n",
        "attn_outputs2 = np.einsum('bqk,bkd -> bqd', attn_weights, V1)  # q^T K V\n",
        "attn_outputs = attn_outputs.transpose(1, 0, 2, 3).reshape(b, n*s, d)  # merge blocks for comparison\n",
        "assert np.allclose(attn_outputs, attn_outputs2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can complete the implementation of a block-wise transformer layer by adding a dense network with residual connections and layer normalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "w1 = np.random.standard_normal((d, d))\n",
        "b1 = np.random.standard_normal(d)\n",
        "w2 = np.random.standard_normal((d, d))\n",
        "b2 = np.random.standard_normal(d)\n",
        "\n",
        "def layer_norm(x: np.ndarray):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    return (x - mean) / np.sqrt(variance)\n",
        "\n",
        "def relu(x: np.ndarray): \n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def linear(x: np.ndarray, w: np.ndarray, b: np.ndarray): \n",
        "    return np.einsum('bqd,dw -> bqw', x, w) + b[None, None]\n",
        "\n",
        "def postprocess(x: np.ndarray):\n",
        "    x0 = x\n",
        "    x = layer_norm(x)\n",
        "\n",
        "    # 2-layer feedforward network\n",
        "    x = linear(x, w1, b1)\n",
        "    x = relu(x)\n",
        "    x = linear(x, w2, b2)\n",
        "\n",
        "    # residual connection + layer normalization\n",
        "    x = x0 + x\n",
        "    x = layer_norm(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2, 7, 5)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = []\n",
        "\n",
        "q: np.ndarray\n",
        "for i, q in enumerate(Q):\n",
        "    assert list(q.shape) == [b, s, d]\n",
        "    num = np.zeros((b,s,d))  # initialize numerator\n",
        "    den = np.zeros((b,s))  # initialize denominator\n",
        "    max_i = -np.inf * np.ones((b, s))  # initialize max_i\n",
        "\n",
        "    k: np.ndarray\n",
        "    v: np.ndarray\n",
        "    for j, (k, v) in enumerate(zip(K, V)):\n",
        "        assert list(k.shape) == [b, s, d]\n",
        "        assert list(v.shape) == [b, s, d]\n",
        "        alpha: np.ndarray = np.einsum('bqd,bkd -> bqk', q, k)  # q^T K\n",
        "        prev = max_i\n",
        "        max_i = np.maximum(alpha.max(-1), max_i)  # update max_i\n",
        "        exp_values = np.einsum('bqk,bkd -> bqd', np.exp(alpha - max_i[..., None]), v)  # e^{alpha - max_i}^T v\n",
        "\n",
        "        # update numerator and denominator\n",
        "        num = num * np.exp(prev - max_i)[..., None] + exp_values  \n",
        "        den = den * np.exp(prev - max_i) + np.exp(alpha - max_i[..., None]).sum(-1)\n",
        "\n",
        "    chunk_attn_output = num / den[..., None]\n",
        "    x = postprocess(chunk_attn_output)\n",
        "    outputs.append(x)\n",
        "\n",
        "outputs = np.stack(outputs)\n",
        "outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm equivalence with vanilla Transformer block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_weights: np.ndarray = softmax(np.einsum('bqd,bkd -> bqk', Q1, K1), -1)  # Q^T K\n",
        "assert list(attn_weights.shape) == [b, s * n, s * n]\n",
        "attn_outputs = np.einsum('bqk,bkd -> bqd', attn_weights, V1)  # q^T K V\n",
        "outputs2 = postprocess(attn_outputs)\n",
        "\n",
        "assert np.allclose(\n",
        "    outputs.transpose(1, 0, 2, 3).reshape(b, n*s, d),  # merge blocks for comparison\n",
        "    outputs2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from multiprocessing import Process, Queue\n",
        "\n",
        "def start_host(\n",
        "    index: int,\n",
        "    q: np.ndarray,\n",
        "    k: np.ndarray,\n",
        "    v: np.ndarray,\n",
        "    primary: Queue,\n",
        "    input_queue: Queue,\n",
        "    output_queue: Queue,\n",
        "):\n",
        "    num = np.zeros((b, s, d))  # initialize numerator\n",
        "    den = np.zeros((b, s))  # initialize denominator\n",
        "    max_i = -np.inf * np.ones((b, s))  # initialize max_i\n",
        "\n",
        "    for _ in range(n):\n",
        "        k, v = input_queue.get()  # Receive k, v from the previous host\n",
        "        assert k.shape == (b, s, d)\n",
        "        assert v.shape == (b, s, d)\n",
        "        alpha = np.einsum(\"bqd,bkd -> bqk\", q, k)  # q^T K\n",
        "        prev = max_i\n",
        "        max_i = np.maximum(alpha.max(-1), max_i)  # update max_i\n",
        "        exp_values = np.einsum(\n",
        "            \"bqk,bkd -> bqd\", np.exp(alpha - max_i[..., None]), v\n",
        "        )  # e^{alpha - max_i}^T v\n",
        "\n",
        "        # update numerator and denominator\n",
        "        num = num * np.exp(prev - max_i)[..., None] + exp_values\n",
        "        den = den * np.exp(prev - max_i) + np.exp(alpha - max_i[..., None]).sum(-1)\n",
        "\n",
        "        output_queue.put((k, v))  # Send k, v to the next host\n",
        "\n",
        "    x = num / den[..., None]\n",
        "    x = postprocess(x)\n",
        "    primary.put((index, x))\n",
        "\n",
        "\n",
        "def ring_transformer():\n",
        "    primary = Queue()\n",
        "    num_hosts = len(Q)\n",
        "    queues = [Queue() for _ in range(num_hosts)]\n",
        "    processes = []\n",
        "\n",
        "    # Create processes\n",
        "    for i, (q, k, v) in enumerate(zip(Q, K, V)):\n",
        "        input_queue = queues[i - 1]  # Previous host queue\n",
        "        output_queue = queues[i]  # Current host queue\n",
        "        process = Process(\n",
        "            target=start_host,\n",
        "            args=(i, q, k, v, primary, input_queue, output_queue),\n",
        "        )\n",
        "        processes.append(process)\n",
        "\n",
        "    # Start processes\n",
        "    for process in processes:\n",
        "        process.start()\n",
        "\n",
        "    # Send initial messages to start the communication\n",
        "    for queue, k, v in zip(queues, K, V):\n",
        "        queue.put((k, v))\n",
        "\n",
        "    # Wait for all processes to complete\n",
        "    for process in processes:\n",
        "        process.join()\n",
        "\n",
        "    # Collect outputs\n",
        "    outputs = sorted([primary.get() for _ in range(num_hosts)])\n",
        "    return np.stack([x for _, x in outputs])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'start_host' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'start_host' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'start_host' on <module '__main__' (built-in)>\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attn_outputs3 \u001b[38;5;241m=\u001b[39m \u001b[43mring_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[1;32m      3\u001b[0m     outputs,\n\u001b[1;32m      4\u001b[0m     outputs3\n\u001b[1;32m      5\u001b[0m )\n",
            "Cell \u001b[0;32mIn[18], line 67\u001b[0m, in \u001b[0;36mring_transformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     process\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Collect outputs\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mprimary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_hosts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack([x \u001b[38;5;28;01mfor\u001b[39;00m _, x \u001b[38;5;129;01min\u001b[39;00m outputs])\n",
            "Cell \u001b[0;32mIn[18], line 67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m     process\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Collect outputs\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([\u001b[43mprimary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hosts)])\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack([x \u001b[38;5;28;01mfor\u001b[39;00m _, x \u001b[38;5;129;01min\u001b[39;00m outputs])\n",
            "File \u001b[0;32m/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/queues.py:103\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock:\n\u001b[0;32m--> 103\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sem\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
            "File \u001b[0;32m/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/connection.py:430\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 430\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m/nix/store/pwr22740f2pv36q5g28l1gjdg1bw43zm-python3-3.11.7/lib/python3.11/multiprocessing/connection.py:395\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    393\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 395\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "outputs3 = ring_transformer()\n",
        "assert np.allclose(\n",
        "    outputs,\n",
        "    outputs3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
