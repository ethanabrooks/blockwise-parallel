{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RlNfQLi3H02"
      },
      "source": [
        "Given $q \\in \\mathbb{R}^d, K \\in \\mathbb{R}^{s \\times d}, V \\in \\mathbb{R}^{s \\times d}$, we define\n",
        "$$\\alpha_i = q^TK_i$$\n",
        "$$\\text{Attn}(q, K, V) = \\frac{e^{\\alpha_1} V_1 + \\cdots + e^{\\alpha_n} V_n}{e^{\\alpha_1} + \\cdots + e^{\\alpha_n}}$$\n",
        "We can compute $\\text{Attn}(q, K, V)$ iteratively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "s9riHhPN3JSf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "d = 5\n",
        "s = 7\n",
        "\n",
        "np.random.seed(0)\n",
        "q = np.random.random(d)\n",
        "K = np.random.random((s, d))\n",
        "V = np.random.random((s, d))\n",
        "\n",
        "num = 0\n",
        "den = 0\n",
        "\n",
        "for k, v in zip(K, V):\n",
        "  alpha = (q * k).sum()\n",
        "  num += np.exp(alpha) * v  # e^{alpha_i}V_i\n",
        "  den += np.exp(alpha)  # e^{alpha_i}\n",
        "\n",
        "attn_output = num / den\n",
        "# e^{alpha_1}V_1 + ... + e^{alpha_n}V_n\n",
        "# -------------------------------------\n",
        "#    e^{alpha_1} + ... + e^{alpha_n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51s8adIA6mvj"
      },
      "source": [
        "This is equivalent to Blockwise Parallel, applied to a single query vector, with a chunk-size of 1 and without the $\\max_i$ term. We can check that `attn_output` is equal to traditional Attn(Q,K,V) computation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVgWzioc4cVP",
        "outputId": "6efcdb69-266e-4293-ac53-53167382255e"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "attn_weights = softmax(np.einsum('d,sd -> s', q, K), -1)  # q^T K\n",
        "attn_output2 = np.einsum('s,sd -> d', attn_weights, V) # q^T K\n",
        "assert np.allclose(attn_output, attn_output2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQeV56VC61Rc"
      },
      "source": [
        "Next we introduce $\\max_i$ to improve floating point stability. In order to show the math more clearly, we will walk through the first two steps of the iteration, using numeric suffixes to prevent variables from shadowing each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pJX6NlFz5R45"
      },
      "outputs": [],
      "source": [
        "num0 = 0\n",
        "den0 = 0\n",
        "max0_i = -np.inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the logic that we perform in the first iteration of the loop, comparing the query vector with the first (size-1) chunk of the key matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHWIJmjP6-uk",
        "outputId": "43213b03-38a2-4861-98ed-a3cf70145663"
      },
      "outputs": [],
      "source": [
        "v0 = V[0]\n",
        "k0 = K[0]\n",
        "alpha0 = (q * k0).sum()\n",
        "max1_i = max(alpha0, max0_i)\n",
        "num1 = num0 * np.exp(max0_i - max1_i) + np.exp(alpha0 - max1_i) * v0\n",
        "assert np.all(num1 == np.exp(alpha0 - max1_i) * v0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2i7cbYJ8xoi"
      },
      "source": [
        "Since $\\texttt{num0} = 0$,\n",
        "$\\texttt{num1} = e^{\\alpha_0 - \\max0_i}v_0$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGczEdA47OVz",
        "outputId": "d54fb96a-9014-45db-c75f-6b785b8819cc"
      },
      "outputs": [],
      "source": [
        "den1 = den0 * np.exp(max0_i - max1_i) + np.exp(alpha0 - max1_i)\n",
        "assert den1 == np.exp(alpha0 - max1_i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E7Z4YeQ8Tri"
      },
      "source": [
        "Similarly, since $\\texttt{den0} = 0$, $\\texttt{den1} = e^{\\alpha_0 - \\max1_i}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSU3Wlic77TJ",
        "outputId": "c0bfbc0b-e760-45d4-ba24-e990afa88563"
      },
      "outputs": [],
      "source": [
        "v1 = V[1]\n",
        "k1 = K[1]\n",
        "alpha1 = (q * k1).sum()\n",
        "max_i2 = max(alpha1, max1_i)\n",
        "num2 = num1 * np.exp(max1_i - max_i2) + np.exp(alpha1 - max_i2) * v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Substituting in the value of $\\texttt{num1}$, we get:\n",
        "$$\\texttt{num2} = e^{\\alpha_0 - \\max0_i}v_0 \\times e^{\\max0_i - \\max1_i} + e^{\\alpha_1 - \\max1_i}v_1$$\n",
        "And simplifying the exponents:\n",
        "$$ = e^{\\alpha_0 - \\max1_i}v_0 + e^{\\alpha_1 - \\max1_i}v_1$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert np.all(\n",
        "    num2 == np.exp(alpha0 - max_i2) * v0 + np.exp(alpha1 - max_i2) * v1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvWl8qvR8DOo",
        "outputId": "d10bff61-96f4-44db-b982-ff1e01dbc58b"
      },
      "outputs": [],
      "source": [
        "den2 = den1 * np.exp(max1_i - max_i2) + np.exp(alpha1 - max_i2)\n",
        "assert den2 == np.exp(alpha0 - max_i2) + np.exp(alpha1 - max_i2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This second equality comes from the fact that when we substite in the value of $\\texttt{den1}$, we get:\n",
        "$$\\texttt{den2} = e^{\\alpha_0 - \\max0_i} \\times e^{\\max0_i - \\max1_i} + e^{\\alpha_1 - \\max1_i}$$\n",
        "And again simplifying the exponents:\n",
        "$$ = e^{\\alpha_0 - \\max1_i} + e^{\\alpha_1 - \\max1_i}$$\n",
        "\n",
        "When we take tha fraction of $\\texttt{num2}$ and $\\texttt{den2}$, the $\\max1_i$ terms cancel out:\n",
        " $$\\frac{\\text{num2}}{\\text{den2}} \n",
        " = \\frac{e^{\\alpha_0 - \\max_i}v_0 + e^{\\alpha_1 - \\max_i}v_1}{e^{\\alpha_0 - \\max_i} + e^{\\alpha_1 - \\max_i}}\n",
        " = \\frac{e^{\\alpha_0} v_0 + e^{\\alpha_1} v_1}{e^{\\alpha_0} + e^{\\alpha_1}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIlrfDq9DYV",
        "outputId": "3b1358cd-5d2b-420d-d28c-a7f9005cbbbc"
      },
      "outputs": [],
      "source": [
        "assert np.allclose(\n",
        "    num2 / den2,\n",
        "    (np.exp(alpha0) * v0 + np.exp(alpha1) * v1) / (np.exp(alpha0) + np.exp(alpha1))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that this previous expression looks like traditional Attn(Q,K,V). We can confirm the equivalence as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgSFJ05QRV8Z",
        "outputId": "2160e773-3a51-4b70-d48d-f4422adf62ec"
      },
      "outputs": [],
      "source": [
        "alpha = np.array([alpha0, alpha1])\n",
        "v = np.array([v0, v1])\n",
        "assert np.allclose(\n",
        "    num2 / den2,\n",
        "    (softmax(alpha)[..., None] * v).sum(0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " $$\\frac{\\text{num2}}{\\text{den2}} = \\text{softmax}(\\alpha_0, \\alpha_1)^T[ v_0, v_1 ]$$\n",
        "\n",
        "This simple example is intended to clarify the equivalence between Blockwise Parallel and traditional Attn(Q,K,V) computation. For the next version, we add the following logic:\n",
        " - chunk-size > 1\n",
        " - iteration over a query matrix\n",
        " - a batch dimension\n",
        "\n",
        "We also put this logic into a loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "lgYpN68O2Jvu"
      },
      "outputs": [],
      "source": [
        "num0 = 0\n",
        "den0 = 0\n",
        "max_i0 = -np.inf\n",
        "n = 3  # number of chunks\n",
        "b = 2  # batch dimension (could also include head dimension, since heads are parallel for self-attention)\n",
        "s = 7\n",
        "d = 5\n",
        "Q = np.random.random((n, b, s, d))\n",
        "K = np.random.random((n, b, s, d))\n",
        "V = np.random.random((n, b, s, d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "xZz1vfDM-fdG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2, 7, 5)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn_outputs = []\n",
        "\n",
        "q: np.ndarray\n",
        "for i, q in enumerate(Q):\n",
        "  assert list(q.shape) == [b, s, d]\n",
        "  num = np.zeros((b,s,d))  # initialize numerator\n",
        "  den = np.zeros((b,s))  # initialize denominator\n",
        "  max_i = -np.inf * np.ones((b, s))  # initialize max_i\n",
        "\n",
        "  k: np.ndarray\n",
        "  v: np.ndarray\n",
        "  for j, (k, v) in enumerate(zip(K, V)):\n",
        "    assert list(k.shape) == [b, s, d]\n",
        "    assert list(v.shape) == [b, s, d]\n",
        "    alpha: np.ndarray = np.einsum('bqd,bkd -> bqk', q, k)  # q^T K\n",
        "    prev = max_i\n",
        "    max_i = np.maximum(alpha.max(-1), max_i)  # update max_i\n",
        "    exp_values = np.einsum('bqk,bkd -> bqd', np.exp(alpha - max_i[..., None]), v)  # e^{alpha - max_i}^T v\n",
        "\n",
        "    # update numerator and denominator\n",
        "    num = num * np.exp(prev - max_i)[..., None] + exp_values  \n",
        "    den = den * np.exp(prev - max_i) + np.exp(alpha - max_i[..., None]).sum(-1)\n",
        "\n",
        "  attn_outputs.append(num / den[..., None])\n",
        "\n",
        "attn_outputs = np.stack(attn_outputs)\n",
        "attn_outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compare this to a traditional Attn(Q,K,V) computation to verify that the two are equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiaKEIkR-y_6",
        "outputId": "14d9c694-1455-41f6-ff63-dd97b49f8f01"
      },
      "outputs": [],
      "source": [
        "Q1 = Q.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "K1 = K.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "V1 = V.transpose([1, 0, 2, 3]).reshape(b, -1, d)\n",
        "attn_weights: np.ndarray = softmax(np.einsum('bqd,bkd -> bqk', Q1, K1), -1)  # Q^T K\n",
        "assert list(attn_weights.shape) == [b, s * n, s * n]\n",
        "attn_outputs2 = np.einsum('bqk,bkd -> bqd', attn_weights, V1)  # q^T K V\n",
        "attn_outputs = attn_outputs.transpose(1, 0, 2, 3).reshape(b, n*s, d)  # merge blocks for comparison\n",
        "assert np.allclose(attn_outputs, attn_outputs2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can complete the implementation of a block-wise transformer layer by adding a dense network with residual connections and layer normalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "w1 = np.random.standard_normal((d, d))\n",
        "b1 = np.random.standard_normal(d)\n",
        "w2 = np.random.standard_normal((d, d))\n",
        "b2 = np.random.standard_normal(d)\n",
        "\n",
        "def layer_norm(x: np.ndarray):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    return (x - mean) / np.sqrt(variance)\n",
        "\n",
        "def relu(x: np.ndarray): \n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def linear(x: np.ndarray, w: np.ndarray, b: np.ndarray): \n",
        "    return np.einsum('bqd,dw -> bqw', x, w) + b[None, None]\n",
        "\n",
        "def postprocess(x: np.ndarray):\n",
        "    x0 = x\n",
        "    x = layer_norm(x)\n",
        "\n",
        "    # 2-layer feedforward network\n",
        "    x = linear(x, w1, b1)\n",
        "    x = relu(x)\n",
        "    x = linear(x, w2, b2)\n",
        "\n",
        "    # residual connection + layer normalization\n",
        "    x = x0 + x\n",
        "    x = layer_norm(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2, 7, 5)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = []\n",
        "\n",
        "q: np.ndarray\n",
        "for i, q in enumerate(Q):\n",
        "    assert list(q.shape) == [b, s, d]\n",
        "    num = np.zeros((b,s,d))  # initialize numerator\n",
        "    den = np.zeros((b,s))  # initialize denominator\n",
        "    max_i = -np.inf * np.ones((b, s))  # initialize max_i\n",
        "\n",
        "    k: np.ndarray\n",
        "    v: np.ndarray\n",
        "    for j, (k, v) in enumerate(zip(K, V)):\n",
        "        assert list(k.shape) == [b, s, d]\n",
        "        assert list(v.shape) == [b, s, d]\n",
        "        alpha: np.ndarray = np.einsum('bqd,bkd -> bqk', q, k)  # q^T K\n",
        "        prev = max_i\n",
        "        max_i = np.maximum(alpha.max(-1), max_i)  # update max_i\n",
        "        exp_values = np.einsum('bqk,bkd -> bqd', np.exp(alpha - max_i[..., None]), v)  # e^{alpha - max_i}^T v\n",
        "\n",
        "        # update numerator and denominator\n",
        "        num = num * np.exp(prev - max_i)[..., None] + exp_values  \n",
        "        den = den * np.exp(prev - max_i) + np.exp(alpha - max_i[..., None]).sum(-1)\n",
        "\n",
        "    chunk_attn_output = num / den[..., None]\n",
        "    x = postprocess(chunk_attn_output)\n",
        "    outputs.append(x)\n",
        "\n",
        "outputs = np.stack(outputs)\n",
        "outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm equivalence with vanilla Transformer block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_weights: np.ndarray = softmax(np.einsum('bqd,bkd -> bqk', Q1, K1), -1)  # Q^T K\n",
        "assert list(attn_weights.shape) == [b, s * n, s * n]\n",
        "attn_outputs = np.einsum('bqk,bkd -> bqd', attn_weights, V1)  # q^T K V\n",
        "outputs2 = postprocess(attn_outputs)\n",
        "\n",
        "assert np.allclose(\n",
        "    outputs.transpose(1, 0, 2, 3).reshape(b, n*s, d),  # merge blocks for comparison\n",
        "    outputs2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we demonstrate a multiprocess implementation of a ring transformer. Because of the use of multiprocessing, the code has to be defined in a separate file. Here, we can see that the logic is still equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from multiprocessing import Queue\n",
        "\n",
        "def start_host(index: int, q: np.ndarray, recv: Queue, send: Queue):\n",
        "    print(f\"Starting host {index}\")\n",
        "    b, s, d = q.shape\n",
        "    num = np.zeros((b, s, d))  # initialize numerator\n",
        "    den = np.zeros((b, s))  # initialize denominator\n",
        "    max_i = -np.inf * np.ones((b, s))  # initialize max_i\n",
        "\n",
        "    for _ in range(n):\n",
        "        k: np.ndarray\n",
        "        v: np.ndarray\n",
        "        k, v = recv.get()  # Receive k, v from the input queue (previous host)\n",
        "        assert k.shape == (b, s, d) and v.shape == (b, s, d)\n",
        "\n",
        "        alpha: np.ndarray = np.einsum(\"bqd,bkd -> bqk\", q, k)  # q^T K\n",
        "        prev = max_i\n",
        "        max_i = np.maximum(alpha.max(-1), max_i)  # update max_i\n",
        "        exp_values = np.einsum(\"bqk,bkd -> bqd\", np.exp(alpha - max_i[..., None]), v)\n",
        "\n",
        "        num = num * np.exp(prev - max_i)[..., None] + exp_values\n",
        "        den = den * np.exp(prev - max_i) + np.exp(alpha - max_i[..., None]).sum(-1)\n",
        "\n",
        "        send.put((k, v))  # Send (k, v) to the output queue for the next host\n",
        "\n",
        "    x = num / den[..., None]\n",
        "    x = postprocess(x)\n",
        "    print(f\"Host {index} done\")\n",
        "    return index, x\n",
        "\n",
        "\n",
        "def ring_transformer():\n",
        "    num_hosts = len(Q)\n",
        "    queues = [\n",
        "        Queue() for _ in range(num_hosts + 1)\n",
        "    ]  # Create queues for each host pair, plus one extra to complete the ring\n",
        "\n",
        "    # Initialize the first set of (k, v) pairs in the queues\n",
        "    for queue, k, v in zip(queues, K, V):\n",
        "        queue.put((k, v))\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_hosts) as executor:\n",
        "        futures = [\n",
        "            executor.submit(start_host, i, q, queues[i], queues[(i + 1) % num_hosts])\n",
        "            for i, q in enumerate(Q)\n",
        "        ]\n",
        "        outputs = [future.result() for future in futures]\n",
        "\n",
        "    # Ensure outputs are sorted by index to maintain deterministic order\n",
        "    return np.stack([x for _, x in sorted(outputs)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting host 0Starting host 1\n",
            "\n",
            "Starting host 2\n",
            "Host 2 done\n",
            "Host 0 done\n",
            "Host 1 done\n"
          ]
        }
      ],
      "source": [
        "outputs3 = ring_transformer()\n",
        "assert np.allclose(\n",
        "    outputs,\n",
        "    outputs3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
